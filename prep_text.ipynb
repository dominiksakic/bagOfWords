{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY55ueoE3K1uxz5trM1AZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/sentimentAnalysisJp/blob/main/prep_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorizing text is the process of transforming text into numeric tensors\n",
        "\n",
        "- standardize, tokenize and convert\n",
        "\n",
        "- 3 different ways to tokenize the standardized text:\n",
        "\n",
        "1. Word-level tokenization\n",
        "2. N-gram tokenization\n",
        "3. Character-level tokenizattion\n",
        "\n",
        "- You can care about word order or not care about it, which is called:\n",
        "1. sequence model (word order)\n",
        "2. bag-of-words models (no word order)\n",
        "\n",
        "\n",
        "# Workflow:\n",
        "1. Text Preprocessing, clean and remove unnecessary characters\n",
        "\n",
        "2. Train Senteice Piece model on the entire dataset -> helpful to tokenize words into subwords\n",
        "\n",
        "3. Tokenize the dataset using the SEntecePiece model.\n",
        "\n",
        "4. Generate Bigrams\n",
        "\n",
        "5. Build Vocabulary\n",
        "\n",
        "6. Generate BoW Vector"
      ],
      "metadata": {
        "id": "V33FvDH3bnl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab indexing\n",
        "\n",
        "## Main idea\n",
        "```\n",
        "vocabulary = {}\n",
        "for text in dataset:\n",
        "  text = standardize(text)\n",
        "  tokens = tokenize(text)\n",
        "  for token in tokens:\n",
        "    if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)\n",
        "\n",
        "def one_hot_encode_token(token):\n",
        "  vector = np.zeros(len(vocabulary),)\n",
        "  token_index = vocabulary[token]\n",
        "  vector[token_index] = 1\n",
        "  return vector\n",
        "```\n",
        "- The result would be a Vector with just one 1 in it."
      ],
      "metadata": {
        "id": "oOcdmzwddAbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi[unidic-lite] -q\n",
        "!pip install sentencepiece -q"
      ],
      "metadata": {
        "id": "r3WkjMwOgAi7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 and 2\n",
        "- no cleaning needed"
      ],
      "metadata": {
        "id": "XoxRWxpmraDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "texts = [\n",
        "    \"このコードは非常に効率的です\",\n",
        "    \"プログラムのデバッグを行います\",\n",
        "    \"新しいアルゴリズムを試しています\",\n",
        "    \"関数のテストを実行します\",\n",
        "    \"データベースに接続しています\",\n",
        "    \"ユーザーインターフェイスを更新しています\",\n",
        "    \"エラーが発生しました\",\n",
        "    \"新しい機能を追加しました\",\n",
        "    \"コードの最適化を行っています\",\n",
        "    \"APIのドキュメントを作成しています\",\n",
        "    \"ユーザーからのフィードバックを受け取っています\",\n",
        "    \"セキュリティ対策を強化しました\",\n",
        "    \"パフォーマンスを向上させました\",\n",
        "    \"プロジェクトの進行状況を確認しています\",\n",
        "    \"バージョン管理を使用しています\",\n",
        "    \"プログラムの動作確認をしています\",\n",
        "    \"コードのリファクタリングを行います\",\n",
        "    \"新しいライブラリをインストールしました\",\n",
        "    \"複雑なデータ構造を処理しています\",\n",
        "    \"メモリ使用量を最適化しています\",\n",
        "    \"コードの可読性を向上させました\",\n",
        "    \"ユニットテストを追加しました\",\n",
        "    \"デバッグログを出力しています\",\n",
        "    \"コードの最終チェックを行っています\"\n",
        "]\n",
        "\n",
        "# Write the texts to a file\n",
        "with open('text_data.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in texts:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "# Train a SentencePiece Model on my text\n",
        "spm.SentencePieceTrainer.train(input='text_data.txt', model_prefix='spm_model', vocab_size=125)\n"
      ],
      "metadata": {
        "id": "-leHZYw1lnJp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example outputs\n",
        "- Vocab\n",
        "- One Hot encoding"
      ],
      "metadata": {
        "id": "5PrRmjjkrpVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "\n",
        "\n",
        "# Get the size of the vocabulary\n",
        "vocab_size = sp.get_piece_size()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Build vocabulary from SentencePiece model\n",
        "vocabulary = {sp.id_to_piece(i): i for i in range(vocab_size)}\n",
        "\n",
        "# Print some sample vocabulary items\n",
        "for token, idx in list(vocabulary.items())[:5]:\n",
        "    print(f\"Token: {token}, Index: {idx}\")\n",
        "\n",
        "def one_hot_encode_token(token, vocabulary):\n",
        "    vector = np.zeros(len(vocabulary), dtype=int)\n",
        "    if token in vocabulary:\n",
        "        vector[vocabulary[token]] = 1\n",
        "\n",
        "    return vector\n",
        "\n",
        "# One-hot encode each token in the vocabulary\n",
        "one_hot_vectors = {token: one_hot_encode_token(token, vocabulary) for token in vocabulary}\n",
        "\n",
        "# Display the one-hot encoding of the first few tokens\n",
        "for token, vector in list(one_hot_vectors.items())[:5]:\n",
        "    print(f\"Token: {token} -> One-Hot Vector: {vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp6ezF9BjL24",
        "outputId": "8140be9c-b355-4eab-dacf-33566b107afd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 125\n",
            "Token: <unk>, Index: 0\n",
            "Token: <s>, Index: 1\n",
            "Token: </s>, Index: 2\n",
            "Token: ま, Index: 3\n",
            "Token: す, Index: 4\n",
            "Token: <unk> -> One-Hot Vector: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: <s> -> One-Hot Vector: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: </s> -> One-Hot Vector: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: ま -> One-Hot Vector: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: す -> One-Hot Vector: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Tokenize Text"
      ],
      "metadata": {
        "id": "35g9PvMNop_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"私はAIが大好きです\",\n",
        "    \"AIは素晴らしい技術です\",\n",
        "    \"日本語のテキストを処理します\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenized_texts = [sp.encode(text, out_type=str) for text in texts]\n",
        "print(f\"Tokenized texts: {tokenized_texts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFwg2YZdo28C",
        "outputId": "47959a61-b3f7-46ae-b254-a70fd5694170"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized texts: [['▁', '私', 'は', 'A', 'I', 'が', '大好き', 'で', 'す'], ['▁', 'A', 'I', 'は', '素晴', 'ら', 'し', 'い', '技術', 'で', 'す'], ['▁', '日本語', 'の', 'テ', 'キ', 'ス', 'ト', 'を', '処', '理', 'し', 'ま', 'す']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 Generate N-Grams"
      ],
      "metadata": {
        "id": "4faKh17sr6yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bigrams(tokens):\n",
        "    return [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "\n",
        "# Generate bigrams for each text\n",
        "bigram_texts = [generate_bigrams(text) for text in tokenized_texts]\n",
        "print(f\"Bigram texts: {bigram_texts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h1ssssYsAxt",
        "outputId": "7af72770-c9dc-4108-9962-4b092cdbc8ad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram texts: [[('▁', '私'), ('私', 'は'), ('は', 'A'), ('A', 'I'), ('I', 'が'), ('が', '大好き'), ('大好き', 'で'), ('で', 'す')], [('▁', 'A'), ('A', 'I'), ('I', 'は'), ('は', '素晴'), ('素晴', 'ら'), ('ら', 'し'), ('し', 'い'), ('い', '技術'), ('技術', 'で'), ('で', 'す')], [('▁', '日本語'), ('日本語', 'の'), ('の', 'テ'), ('テ', 'キ'), ('キ', 'ス'), ('ス', 'ト'), ('ト', 'を'), ('を', '処'), ('処', '理'), ('理', 'し'), ('し', 'ま'), ('ま', 'す')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 Build Vocab\n",
        "- Why build the Vocab form the Bigrams and not the other way around?\n",
        "- The Vocab defines what features the model look at! In a Bigram way, the features are 2 Words"
      ],
      "metadata": {
        "id": "p2jfMoLqsIV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary from bigrams\n",
        "vocabulary = {}\n",
        "for text in bigram_texts:\n",
        "    for bigram in text:\n",
        "        if bigram not in vocabulary:\n",
        "            vocabulary[bigram] = len(vocabulary)\n",
        "\n",
        "print(f\"Vocabulary: {vocabulary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkNLeraDsMp5",
        "outputId": "2a2763b9-e368-402e-d560-307f798e5479"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {('▁', '私'): 0, ('私', 'は'): 1, ('は', 'A'): 2, ('A', 'I'): 3, ('I', 'が'): 4, ('が', '大好き'): 5, ('大好き', 'で'): 6, ('で', 'す'): 7, ('▁', 'A'): 8, ('I', 'は'): 9, ('は', '素晴'): 10, ('素晴', 'ら'): 11, ('ら', 'し'): 12, ('し', 'い'): 13, ('い', '技術'): 14, ('技術', 'で'): 15, ('▁', '日本語'): 16, ('日本語', 'の'): 17, ('の', 'テ'): 18, ('テ', 'キ'): 19, ('キ', 'ス'): 20, ('ス', 'ト'): 21, ('ト', 'を'): 22, ('を', '処'): 23, ('処', '理'): 24, ('理', 'し'): 25, ('し', 'ま'): 26, ('ま', 'す'): 27}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 Generate BoW"
      ],
      "metadata": {
        "id": "aDHtt5J5sUpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bag_of_words_bigrams(text, vocabulary):\n",
        "    vector = np.zeros(len(vocabulary), dtype=int)\n",
        "\n",
        "    # Count the frequency of each bigram in the vocabulary\n",
        "    for bigram in text:\n",
        "        if bigram in vocabulary:\n",
        "            vector[vocabulary[bigram]] += 1\n",
        "\n",
        "    return vector\n",
        "\n",
        "# Generate BoW\n",
        "bow_vectors = [bag_of_words_bigrams(text, vocabulary) for text in bigram_texts]\n",
        "\n",
        "# Display BoW vectors\n",
        "for i, bow_vector in enumerate(bow_vectors):\n",
        "    print(f\"Text {i + 1} BoW: {bow_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZgNIv-ItGjX",
        "outputId": "d609f11b-9f43-4b89-858a-cb0fe8a41155"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1 BoW: [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Text 2 BoW: [0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Text 3 BoW: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare to Unigram output"
      ],
      "metadata": {
        "id": "X7YBpEVOtmxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts = [\n",
        "    [\"私\", \"は\", \"AI\", \"が\", \"大好き\", \"です\"],\n",
        "    [\"AI\", \"は\", \"素晴らしい\", \"技術\", \"です\"],\n",
        "    [\"日本語\", \"の\", \"テキスト\", \"を\", \"処理\", \"します\"]\n",
        "]\n",
        "\n",
        "# Build unigram vocabulary\n",
        "unigram_vocab = {}\n",
        "for text in tokenized_texts:\n",
        "    for token in text:\n",
        "        if token not in unigram_vocab:\n",
        "            unigram_vocab[token] = len(unigram_vocab)\n",
        "\n",
        "print(\"Unigram Vocab:\", unigram_vocab)\n",
        "\n",
        "# Create BoW vectors\n",
        "def unigram_bow_vector(tokens, vocab):\n",
        "    vector = np.zeros(len(vocab), dtype=int)\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            vector[vocab[token]] += 1\n",
        "    return vector\n",
        "\n",
        "unigram_bows = [unigram_bow_vector(text, unigram_vocab) for text in tokenized_texts]\n",
        "\n",
        "for i, bow in enumerate(unigram_bows):\n",
        "    print(f\"Text {i+1} Unigram BoW: {bow}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O2P41dNtpy7",
        "outputId": "58f1cd40-256f-4468-f69a-601d3d60c902"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Vocab: {'私': 0, 'は': 1, 'AI': 2, 'が': 3, '大好き': 4, 'です': 5, '素晴らしい': 6, '技術': 7, '日本語': 8, 'の': 9, 'テキスト': 10, 'を': 11, '処理': 12, 'します': 13}\n",
            "Text 1 Unigram BoW: [1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
            "Text 2 Unigram BoW: [0 1 1 0 0 1 1 1 0 0 0 0 0 0]\n",
            "Text 3 Unigram BoW: [0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n"
          ]
        }
      ]
    }
  ]
}