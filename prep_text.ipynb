{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyM38YY34FPWiWFF7a0kcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/sentimentAnalysisJp/blob/main/prep_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorizing text is the process of transforming text into numeric tensors\n",
        "\n",
        "- standardize, tokenize and convert\n",
        "\n",
        "- 3 different ways to tokenize the standardized text:\n",
        "\n",
        "1. Word-level tokenization\n",
        "2. N-gram tokenization\n",
        "3. Character-level tokenizattion\n",
        "\n",
        "- You can care about word order or not care about it, which is called:\n",
        "1. sequence model (word order)\n",
        "2. bag-of-words models (no word order)"
      ],
      "metadata": {
        "id": "V33FvDH3bnl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab indexing\n",
        "\n",
        "## Main idea\n",
        "```\n",
        "vocabulary = {}\n",
        "for text in dataset:\n",
        "  text = standardize(text)\n",
        "  tokens = tokenize(text)\n",
        "  for token in tokens:\n",
        "    if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)\n",
        "\n",
        "def one_hot_encode_token(token):\n",
        "  vector = np.zeros(len(vocabulary),)\n",
        "  token_index = vocabulary[token]\n",
        "  vector[token_index] = 1\n",
        "  return vector\n",
        "```\n",
        "- The result would be a Vector with just one 1 in it."
      ],
      "metadata": {
        "id": "oOcdmzwddAbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi[unidic-lite] -q\n",
        "!pip install sentencepiece -q"
      ],
      "metadata": {
        "id": "r3WkjMwOgAi7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "texts = [\n",
        "    \"私はAIが大好きです\",\n",
        "    \"AIは素晴らしい技術です\",\n",
        "    \"日本語のテキストを処理します\"\n",
        "]\n",
        "\n",
        "# Write the texts to a file\n",
        "with open('text_data.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in texts:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "# Train a SentencePiece Model on my text\n",
        "spm.SentencePieceTrainer.train(input='text_data.txt', model_prefix='spm_model', vocab_size=34)\n"
      ],
      "metadata": {
        "id": "-leHZYw1lnJp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
        "\n",
        "# Example Japanese text to tokenize\n",
        "text = \"私はAIが大好きです\"\n",
        "\n",
        "# Tokenize the text into subword units\n",
        "tokens = sp.encode(text, out_type=str)\n",
        "\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp6ezF9BjL24",
        "outputId": "2a917cae-5dd9-403f-f489-3b9ee0c78f25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['▁', '私', 'は', 'AI', 'が', '大', '好', 'き', 'で', 'す']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the size of the vocabulary\n",
        "vocab_size = sp.get_piece_size()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Build vocabulary from SentencePiece model\n",
        "vocabulary = {sp.id_to_piece(i): i for i in range(vocab_size)}\n",
        "\n",
        "# Print some sample vocabulary items\n",
        "for token, idx in list(vocabulary.items())[:5]:\n",
        "    print(f\"Token: {token}, Index: {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uag28jbHkEDE",
        "outputId": "7d0be217-359b-4f8e-af7d-a0d1276b722b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 34\n",
            "Token: <unk>, Index: 0\n",
            "Token: <s>, Index: 1\n",
            "Token: </s>, Index: 2\n",
            "Token: ▁, Index: 3\n",
            "Token: す, Index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_token(token, vocabulary):\n",
        "    vector = np.zeros(len(vocabulary), dtype=int)\n",
        "    if token in vocabulary:\n",
        "        vector[vocabulary[token]] = 1\n",
        "\n",
        "    return vector\n",
        "\n",
        "# One-hot encode each token in the vocabulary\n",
        "one_hot_vectors = {token: one_hot_encode_token(token, vocabulary) for token in vocabulary}\n",
        "\n",
        "# Display the one-hot encoding of the first few tokens\n",
        "for token, vector in list(one_hot_vectors.items())[:5]:\n",
        "    print(f\"Token: {token} -> One-Hot Vector: {vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5i7BXhm9gj",
        "outputId": "0c508eb3-8eab-4c66-a573-59b63efced3e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: <unk> -> One-Hot Vector: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: <s> -> One-Hot Vector: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: </s> -> One-Hot Vector: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: ▁ -> One-Hot Vector: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Token: す -> One-Hot Vector: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    }
  ]
}